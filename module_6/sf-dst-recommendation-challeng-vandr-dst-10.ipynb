{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Any results you write to the current directory are saved as output.\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/recommendationsv4/train.csv')\ntest = pd.read_csv('/kaggle/input/recommendationsv4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/recommendationsv4/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"Датасет\n\nУ вас будет история оценок пользователя вместе с его обзором. Вы можете использовать текст рецензии в качестве дополнительной информации. Все оценки пользователей нормированы для бинарной классификации, если человек поставил оценку продукту больше 3 (не включительно), то мы считаем, что продукт ему понравился, если меньше 4, то продукт не понравился.\n\nСоставлю список характеристик признаков train:\n\n* rating - понравилась или не понравилась книга (1 - понравилась, 0 - не понравилась). \n\ntest.csv - набор данных, для которого вы должны сделать предсказания. У каждого наобора userid, itemid есть свой id, для которого вы должны сделать предсказание:\n\n- overall - рейтинг, который поставил пользователь\n- verified - был ли отзыв верифицирован\n- reviewTime - когда был отзыв написан\n- reviewerName - имя пользователя\n- reviewText - текст отзыва\n- summary - сжатый отзыв\n- vote - количество голосований за отзыв\n- style - метаданные\n- image - изображение продукта\n- userid - id пользователя\n- itemid - id товара\n- id - id для предсказания"},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на train"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ASIN является уникальным для листинга продукта, вы его получаете от Амазон вместо UPC кода. Назову это условно \"товар\"."},{"metadata":{},"cell_type":"markdown","source":"# Ок, выделю товары в отдельный список и попробую просмотреть на рейтинги, лайки и тд."},{"metadata":{},"cell_type":"markdown","source":"Создам базу товаров (из теста и трейна) - для этого возьму itemid и asin, а потом сгруппирую по одному из признаков (по идее не важно по какому, это одно и то же, но проверим).\n\nДля будущих интересностей еще сделаю еще несколько колонок в списке товаров - положительные и негативные оценки. Буду ориентироваться  на 1 или 0 от пользователя. Если 0 - негативная оценка. Если 1 - положительная."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None \n\n# возьму нужные колонки из трейна и создам новый датасет с товарами\nitems_train_list = train[['asin', 'itemid', 'rating']]\n\nitems_train_list['likes_up'] = np.where(items_train_list['rating'] == 1, 1, 0)\nitems_train_list['likes_down'] = np.where(items_train_list['rating'] == 0, 1, 0)\n\n# сгруппирую товары по asin и посмотрю, сколько itemid входит\nitems_train_list_grouped_by_likes = items_train_list.groupby(['asin']) \\\n    .agg({'itemid': 'count', 'likes_up': 'sum', 'likes_down': 'sum'}) \\\n    .sort_values(by=['asin'], ascending=False)\\\n    .reset_index()\\\n    .rename(columns={'itemid': 'all_likes'})\n\nitems_train_list_grouped_by_likes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Добавлю также средний рейтинг товара"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_train_list_with_overall = train[['asin', 'itemid', 'overall']]\n\n# сгруппирую товары по asin и посмотрю средний рейтинг\nitems_train_list_grouped_by_overall_mean = items_train_list_with_overall.groupby(['asin']) \\\n    .agg({'overall': 'mean'}) \\\n    .sort_values(by=['asin'], ascending=False) \\\n    .reset_index() \\\n    .rename(columns={'overall': 'item_rating_mean'})\n\nitems_train_list_grouped_by_overall_mean","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Склею два датасета товаров"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_train_list = items_train_list_grouped_by_overall_mean.merge(items_train_list_grouped_by_likes, left_on='asin', right_on='asin')\nitems_train_list = items_train_list.sort_values(by='all_likes', ascending=False)\nitems_train_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на распределение количества лайков и книг"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_train_list['all_likes'].hist(bins='doane')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"items_train_list[items_train_list['all_likes'] > 100]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Что мы видим выше - что только около 1200 товаров из 41000 имеют количество лайков больше 100. Остальные товары можно отнести к LongTail... Что это значит для нас на практике?\n\nЭто значит, что мы можем применить это для \"холодного старта\".\n\nСуществует очень маленькое количество айтемов, о которых знают все. Нет никакого смысла их рекомендовать, потому что пользователь, скорее всего, либо уже их видел и просто не поставил оценку, либо и так о них знает и собирается посмотреть, либо твердо решил не смотреть вовсе. Я не раз смотрел трейлер «Списка Шиндлера», но посмотреть так и не собрался. С другой стороны, популярность очень быстро спадает, и подавляющее количество айтемов практически никто не видел. Делать рекомендации из этой части полезней: там есть интересный контент, который пользователь вряд ли сможет найти сам.\n\nА еще для \"холодного старта\" мы можем на старте как-то опросить пользователя на предмет того, какие ему товары нравятся (выбор из списка), как делает тот же Pinterest (или другие, где со старта мне предлагают оценить мои интересы). Или если мы говорим про товары, то можем ему со старта показать 5 самых популярных наших товаров (которые имеют больше всего голосов), которые он может отметить голосами - нравится или не нравится. И мы уже будем знать, что ему примерно нравится."},{"metadata":{},"cell_type":"markdown","source":"Как не сортировать по среднему рейтингу и учитывать количество оценок? Посчитать доверительный интервал: «Исходя из имеющихся оценок, с вероятностью в 95 % истинная доля положительных оценок как минимум какая?». Ответ на этот вопрос дал Эдвин Уилсон в 1927 году: http://www.evanmiller.org/how-not-to-sort-by-average-rating.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sqrt\n\ndef confidence(ups, downs):\n    n = ups + downs\n\n    if n == 0:\n        return 0\n\n    z = 1.96 #1.44 = 85%, 1.96 = 95%\n    phat = float(ups) / n\n    return ((phat + z*z/(2*n) - z * sqrt((phat*(1-phat)+z*z/(4*n))/n))/(1+z*z/n))\n\nitems_train_list['item_score'] = items_train_list.apply(lambda row : confidence(row['likes_up'], row['likes_down']), axis = 1)\n\nitems_train_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим-ка на распределение score"},{"metadata":{"trusted":true},"cell_type":"code","source":"items_train_list['item_score'].hist(bins='doane')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Довольно интересное распределение. С натяжкой вроде бы можно отнести к нормальному распределению."},{"metadata":{},"cell_type":"markdown","source":"Пока что провел некоторые операции с базой товаров - это больше интересно для холодного старта, ориентируясь на рейтинги товара. Пока что я вижу так. А теперь продолжим работать с пользователями, лайками из датасета.."},{"metadata":{},"cell_type":"markdown","source":"# Работа с train"},{"metadata":{},"cell_type":"markdown","source":"### Image"},{"metadata":{},"cell_type":"markdown","source":"Сразу видно, что в колонке image очень много пропусков. Побороть это не представляется возможным. Но так как это только 1% от общей выборки, то можно смело удалить признак."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='image')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### unixReviewTime, reviewTime"},{"metadata":{},"cell_type":"markdown","source":"Проверю, что выдает unixReviewTime при переводе его в нормальный формат времени."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['reviewTime_new'] = pd.to_datetime(train.unixReviewTime, unit='s')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Видим, что новая колонка полностью дублирует колонку reviewTime, но уже в нормальном формате. Следовательно, теперь reviewTime можно смело удалить."},{"metadata":{},"cell_type":"markdown","source":"Удалять буду так, как того требует команда pandas путем замены текущего датафрейма, без inplace=True"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns=['reviewTime'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"И сразу переименую колонку reviewTime_new в нормальное название"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.rename(columns={'reviewTime_new': 'review_time', 'unixReviewTime': 'unix_review_time'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rating"},{"metadata":{},"cell_type":"markdown","source":"Выяснили, что rating - это нравится книга или нет. Переведу это в int."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['rating'] = train['rating'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрю пристальнее на признаки. Создам функцию просмотра данных:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def col_info(col, bins):\n    print('Количество пропусков: {}'.format(col.isnull().sum()))\n    print('{},'.format(col.describe()))\n    print('Распределение:\\n{},'.format(col.value_counts()))\n    col.hist(bins=bins)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overall"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_info(train.overall, 'doane')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Переведу в int признак overall."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['overall'] = train['overall'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Verified"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.verified.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.verified.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Что это зна признак пока не понятно. Пропусков нет. Варианты - True или False. Есть предположение, что речь идет о том, что отзыв был заапрувлен (опубликован на сайте) или нет."},{"metadata":{},"cell_type":"markdown","source":"Надо ли ориентироваться на этот признак? В сущности, человек так или иначе оставил свой отзыв и это его мнение о книге. Значит, можно удалить и этот признак."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='verified')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vote"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_info(train.vote, 'doane')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Очень много пропусков... Сделать с этим ничего нельзя - удалю признак."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='vote')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### reviewerName\t"},{"metadata":{},"cell_type":"markdown","source":"Думаю, что нет смысла ориентироваться на этот признак - он ничего нам не дает."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='reviewerName')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Попробую добавить новые признаки на основании даты отзыва - год, месяц, день, день недели."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['review_year'] = pd.DatetimeIndex(train['review_time']).year\ntrain['review_month'] = pd.DatetimeIndex(train['review_time']).month\ntrain['review_day'] = pd.DatetimeIndex(train['review_time']).day\ntrain['review_dayofweek'] = pd.DatetimeIndex(train['review_time']).dayofweek.astype(object)\n\n# перенесу этот признак наперед, потому что он у меня объект и мне будет удобнее провести OneEncoding для CatBoost\ncols = ['review_dayofweek']  + [col for col in train if col != 'review_dayofweek']\ntrain = train[cols]\n\ntrain = train.drop(columns='review_time')\ntrain","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Так как в тестовой выборке нет колонки overall, удалю ее и в трейне"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='overall')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### style"},{"metadata":{},"cell_type":"markdown","source":"Тоже самое с колонкой style"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns='style')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### reviewText, summary"},{"metadata":{},"cell_type":"markdown","source":"Так как в тесте нету reviewText, summary, то удалю их - все равно не смогу применить эти признаки."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(columns=['summary', 'reviewText'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Работа с test"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/recommendationsv4/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop(columns='image')\ntest['reviewTime_new'] = pd.to_datetime(test.unixReviewTime, unit='s')\ntest = test.drop(columns=['reviewTime'])\ntest = test.rename(columns={'reviewTime_new': 'review_time', 'unixReviewTime': 'unix_review_time'})\ntest = test.drop(columns=['vote', 'reviewerName'])\ntest = test.drop(columns='verified')\ntest['review_year'] = pd.DatetimeIndex(test['review_time']).year\ntest['review_month'] = pd.DatetimeIndex(test['review_time']).month\ntest['review_day'] = pd.DatetimeIndex(test['review_time']).day\ntest['review_dayofweek'] = pd.DatetimeIndex(test['review_time']).dayofweek.astype(object)\n\n# перенесу этот признак наперед, потому что он у меня объект и мне будет удобнее провести OneEncoding для CatBoost\ncols = ['review_dayofweek']  + [col for col in test if col != 'review_dayofweek']\ntest = test[cols]\n\ntest = test.drop(columns='review_time')\ntest = test.drop(columns='Id')\ntest = test.drop(columns='style')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit/Predict"},{"metadata":{},"cell_type":"markdown","source":"Ну что же, первичные преобразование сделал. Попробую теперь получить какое-то предсказание с помощью LightFM - это будет мой бейслайн."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.sparse as sparse\n\nfrom lightfm import LightFM\nfrom lightfm.cross_validation import random_train_test_split\nfrom lightfm.evaluation import auc_score, precision_at_k, recall_at_k\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n\nimport scipy.sparse as sparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data = train_test_split(train,random_state=32, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_coo = sparse.coo_matrix((train_data['rating'].astype(int),\n                                 (train_data['userid'],\n                                  train_data['itemid'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_THREADS = 4 #число потоков\nNUM_COMPONENTS = 70 #число параметров вектора \nNUM_EPOCHS = 20 #число эпох обучения\n\nmodel = LightFM(learning_rate=0.1, loss='logistic',\n                learning_schedule='adagrad',\n                no_components=NUM_COMPONENTS)\nmodel = model.fit(ratings_coo, epochs=NUM_EPOCHS, \n                  num_threads=NUM_THREADS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lightfm = model.predict(test_data.userid.values,\n                      test_data.itemid.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lightfm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lightfm.min(), preds_lightfm.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds_lightfm = (preds_lightfm - preds_lightfm.min())/(preds_lightfm - preds_lightfm.min()).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds_lightfm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sklearn.metrics.roc_auc_score(test_data.rating,preds_lightfm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds_lightfm.min(), normalized_preds_lightfm.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lightfm = model.predict(test.userid.values,\n                      test.itemid.values)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_lightfm.min(), preds_lightfm.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_normalized_preds_lightfm = (preds_lightfm - preds_lightfm.min())/(preds_lightfm - preds_lightfm.min()).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds_lightfm.min(), normalized_preds_lightfm.max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict Upper"},{"metadata":{},"cell_type":"markdown","source":"Попробую теперь улучшить модель. Для этого буду вводить новые признаки в датафреймы и с помощью различных ухищрений и библиотеки CatBoost попробую улучшить прогноз для колонки rating."},{"metadata":{},"cell_type":"markdown","source":"Добавлю к таблицам трейн и тест данные из таблицы товаров"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test.copy()\ntrain_full = train.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CatBoost выдает значительно хуже результат, если в данные не добавить значения о книгах."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full[['unix_review_time', 'userid', 'itemid', 'rating', 'review_year', 'review_month', 'review_day']] = train_full[['unix_review_time', 'userid', 'itemid', 'rating', 'review_year', 'review_month', 'review_day']].astype('int32')\ntest_full[['unix_review_time', 'userid', 'itemid', 'review_year', 'review_month', 'review_day']] = test_full[['unix_review_time', 'userid', 'itemid', 'review_year', 'review_month', 'review_day']].astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = train.merge(items_train_list, how='left', left_on='asin', right_on='asin')\ntest_full = test.merge(items_train_list, how='left', left_on='asin', right_on='asin')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"После того как добавил в тестовый датасет данные из колонки товары, выяснилось, что в тесте нет данных для 60 товаров. На фоне общего количества это мизер. Поэтому просто добавлю сюда срение значения по выборе и все."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full['item_rating_mean'].fillna((test_full['item_rating_mean'].mean()), inplace=True)\ntest_full['all_likes'].fillna((test_full['all_likes'].mean()), inplace=True)\ntest_full['likes_up'].fillna((test_full['likes_up'].mean()), inplace=True)\ntest_full['likes_down'].fillna((test_full['likes_down'].mean()), inplace=True)\ntest_full['item_score'].fillna((test_full['item_score'].mean()), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ранее с помощью K-FOLD техними получил с помощью LightFM предсказания для всех трейновых записей. Чтобы не засорять страницу с кодом, вынес это в отдельный файл. Теперь подключил его и далее добавлю данные в датасеты для второй модели."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target_predict_from_lightfm = pd.read_csv('../input/train-target-predict/train_target_predict_from_lightfm.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь добавлю предсказания, полученные от LightFM для каждого наблюдения. И буду на них тренировать вторую модель."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full['lightfm_rating'] = train_target_predict_from_lightfm\n\ntest_full['lightfm_rating'] = pd.DataFrame(test_normalized_preds_lightfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Нам по ходу не нужен признак itemid, так как оно коррелирует с asin. А зачем userid? Тоже вроде не нужен"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test_full.drop(columns=['itemid'])\ntrain_full = train_full.drop(columns=['itemid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test_full.drop(columns=['asin'])\ntrain_full = train_full.drop(columns=['asin'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test_full.drop(columns=['itemid', 'asin', 'all_likes', 'likes_up', 'likes_down'])\ntrain_full = train_full.drop(columns=['itemid', 'asin', 'all_likes', 'likes_up', 'likes_down'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Попробую сгенерировать признаки на основании временного окна"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full['lightfm_rolling_mean'] = train_full['lightfm_rating'].rolling(window=7).mean()\ntrain_full['lightfm_rolling_mean'] = train_full['lightfm_rolling_mean'].fillna(0)\n\ntest_full['lightfm_rolling_mean'] = test_full['lightfm_rating'].rolling(window=7).mean()\ntest_full['lightfm_rolling_mean'] = test_full['lightfm_rolling_mean'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full['lightfm_expanding_mean'] = train_full['lightfm_rating'].expanding(2).mean()\ntrain_full['lightfm_expanding_mean'] = train_full['lightfm_expanding_mean'].fillna(0)\n\ntest_full['lightfm_expanding_mean'] = test_full['lightfm_rating'].expanding(2).mean()\ntest_full['lightfm_expanding_mean'] = test_full['lightfm_expanding_mean'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Этот код выбирает числовые признаки, вычисляет их логарифмы, выбирает два категориальных признака, применяет к ним one-hot кодирование и объединяет оба множества в одно, а таже суммирует значения признаков между собой."},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\ndef itr_combinations_new_features(data):\n    \n    tmp_df = data.copy()\n    \n    old_columns = [item for item in tmp_df.columns.tolist() if item != 'rating' and item != 'review_dayofweek' and item != 'userid']\n    \n    tmp_df_sum_2 = [(pd.Series(tmp_df.loc[:,list(i)].sum(axis=1),\\\n            name='_sum2_'.join(tmp_df.loc[:,list(i)].columns))) for i in list(itertools.combinations(old_columns,2))]\n    \n    tmp_df_sum_3 = [(pd.Series(tmp_df.loc[:,list(i)].sum(axis=1),\\\n            name='_sum3_'.join(tmp_df.loc[:,list(i)].columns))) for i in list(itertools.combinations(old_columns,3))]\n    \n    tmp_df_multiplication = [(pd.Series(tmp_df.loc[:,i[0]]*tmp_df.loc[:,i[1]],\\\n            name='_mult_'.join(tmp_df.loc[:,list(i)].columns))) for i in list(itertools.combinations(old_columns,2))]\n    \n   \n    df_sum_2 = pd.DataFrame(tmp_df_sum_2).T  \n    \n    df_sum_3 = pd.DataFrame(tmp_df_sum_3).T  \n    \n    df_multiplication = pd.DataFrame(tmp_df_multiplication).T  \n   \n    data = pd.concat([data, df_sum_2, df_sum_3, df_multiplication], axis=1, sort=False)         \n \n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = itr_combinations_new_features(train_full)\ntrain_full = train_full.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = itr_combinations_new_features(test_full)\ntest_full = test_full.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def code_mean(data, cat_feature, real_feature):\n    \"\"\"\n    Returns a dictionary where keys are unique categories of the cat_feature,\n    and values are means over real_feature\n    \"\"\"\n    return dict(data.groupby(cat_feature)[real_feature].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"average_dayofweek = code_mean(train_full, 'review_day', \"rating\")\nplt.figure(figsize=(7, 5))\nplt.title(\"dayofweek averages\")\npd.DataFrame.from_dict(average_dayofweek, orient='index')[0].plot()\nplt.grid(True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full[\"lightfm_rating_by_day_average\"] = list(map(code_mean(train_full, 'review_day', \"lightfm_rating\").get, train_full.review_day))\ntest_full[\"lightfm_rating_by_day_average\"] = list(map(code_mean(test_full, 'review_day', \"lightfm_rating\").get, test_full.review_day))\n\ntrain_full[\"review_by_month_average\"] = list(map(code_mean(train_full, 'review_month', \"lightfm_rating\").get, train_full.review_month))\ntest_full[\"review_by_month_average\"] = list(map(code_mean(test_full, 'review_month', \"lightfm_rating\").get, test_full.review_month))\n\ntrain_full[\"lightfm_rating_by_dayofweek_average\"] = list(map(code_mean(train_full, 'review_dayofweek', \"lightfm_rating\").get, train_full.review_dayofweek))\ntest_full[\"lightfm_rating_by_dayofweek_average\"] = list(map(code_mean(test_full, 'review_dayofweek', \"lightfm_rating\").get, test_full.review_dayofweek))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подчищу уже ненужные признаки"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test_full.drop(columns=['review_year', 'review_month', 'review_day', 'unix_review_time'])\ntrain_full = train_full.drop(columns=['review_year', 'review_month', 'review_day', 'unix_review_time'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full[['lightfm_rating', 'lightfm_rolling_mean', 'lightfm_expanding_mean', 'lightfm_rating_by_day_average', 'review_by_month_average', 'lightfm_rating_by_dayofweek_average']] = train_full[['lightfm_rating', 'lightfm_rolling_mean', 'lightfm_expanding_mean', 'lightfm_rating_by_day_average', 'review_by_month_average', 'lightfm_rating_by_dayofweek_average']].astype('float32')\ntest_full[['lightfm_rating', 'lightfm_rolling_mean', 'lightfm_expanding_mean', 'lightfm_rating_by_day_average', 'review_by_month_average', 'lightfm_rating_by_dayofweek_average']] = test_full[['lightfm_rating', 'lightfm_rolling_mean', 'lightfm_expanding_mean', 'lightfm_rating_by_day_average', 'review_by_month_average', 'lightfm_rating_by_dayofweek_average']].astype('float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_info(train_full.lightfm_rating, 'doane')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_info(train_full.item_score, 'doane')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Плохо распределен признак item_score. Логарифмировать не получится. Надо подумать, что можно сделать."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Применю теперь модель CatBoost Regressor"},{"metadata":{},"cell_type":"markdown","source":"Но для начала удалю из трейна колонку rating, так как по сути теперь учить модель я буду на тех результатах, которые предсказал LightFM. Ну и в тесте удалю этот признак - его и будем учить."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = test_full.drop(columns=['lightfm_rating', 'userid'])\ntrain_full = train_full.drop(columns=['rating', 'userid'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train_full.drop(['lightfm_rating'], axis=1,)\ny = train_full.lightfm_rating.values\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75, random_state=42)\n\nX_test = test_full.drop([], axis=1,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Байесовская оптимизация параметров CatBoost с Hyperopt"},{"metadata":{"trusted":true},"cell_type":"code","source":"import hyperopt\n\ndef hyperopt_objective(params):\n    model = CatBoostRegressor(\n        l2_leaf_reg=int(params['l2_leaf_reg']),\n        learning_rate=params['learning_rate'],\n        iterations=100,\n        eval_metric='AUC',\n        random_seed=42,\n        verbose=False,\n        loss_function='RMSE',\n    )\n    \n    cv_data = cv(\n        Pool(X_train, y_train),\n        model.get_params()\n    )\n    best_accuracy = np.max(cv_data['test-AUC-mean'])\n    \n    return 1 - best_accuracy # as hyperopt minimises","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy.random import RandomState\n\nparams_space = {\n    'l2_leaf_reg': hyperopt.hp.qloguniform('l2_leaf_reg', 0, 2, 1),\n    'learning_rate': hyperopt.hp.uniform('learning_rate', 0.01, 0.5),\n}\n\ntrials = hyperopt.Trials()\n\nbest = hyperopt.fmin(\n    hyperopt_objective,\n    space=params_space,\n    algo=hyperopt.tpe.suggest,\n    max_evals=10,\n    trials=trials,\n    rstate=RandomState(123)\n)\n\nprint(best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CatBoost Regression\n\nparams = {\n    'iterations': 400,\n    'learning_rate': 0.2,\n    'l2_leaf_reg': 3,\n    'eval_metric': 'AUC',\n    'loss_function': 'RMSE',\n    'use_best_model': True,\n    'random_seed': 42,\n    'logging_level': 'Silent'\n}\n\n\nmodel = CatBoostRegressor(**params)\n\ncategorical_features_names = [0]\n\n# Fit model\nmodel.fit(\n    X_train, y_train,\n    cat_features=categorical_features_names,\n    eval_set=(X_validation, y_validation),\n#     logging_level='Verbose',  # you can uncomment this for text output\n    plot=True\n);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим, какие признаки CatBoost считает важными."},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor, cv\n\nfeature_score = pd.DataFrame(list(zip(X.dtypes.index, model.get_feature_importance(Pool(X_train, label=y_train, cat_features=categorical_features_names)))),\n                columns=['Feature','Score'])\n\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nfeature_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Удалю ненужные по-моему мнению признаки."},{"metadata":{"trusted":true},"cell_type":"code","source":"def del_bag_feat(data):\n\n    feature_score_upd = feature_score[(feature_score['Score']<0.3)]\n    feature_score_upd = feature_score_upd['Feature'].tolist()\n    \n    print(feature_score_upd)\n    \n    indexes_to_keep = set(data.columns) - set(feature_score_upd)    \n   \n    data = data[indexes_to_keep]\n    \n\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full2 = del_bag_feat(train_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full2 = del_bag_feat(test_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"А теперь удалю еще признаки, которые коррелируют друг с другом."},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_corr_features(data):\n    # Create correlation matrix\n    corr_matrix = data.corr().abs()\n\n    # Select upper triangle of correlation matrix\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n    # Find features with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n    print(to_drop)\n    \n    data.drop(to_drop, axis=1, inplace=True)\n\n    # Drop features \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full = drop_corr_features(train_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full = drop_corr_features(test_full)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_full","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# перенесу этот признак наперед, потому что он у меня объект и мне будет удобнее провести OneEncoding для CatBoost\ncols = ['review_dayofweek']  + [col for col in train_full if col != 'review_dayofweek']\ntrain_full = train_full[cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# перенесу этот признак наперед, потому что он у меня объект и мне будет удобнее провести OneEncoding для CatBoost\ncols = ['review_dayofweek']  + [col for col in test_full if col != 'review_dayofweek']\ntest_full = test_full[cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Запущу теперь заново обучение с уже почищенными данными."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train_full2.drop(['lightfm_rating'], axis=1,)\ny = train_full2.lightfm_rating.values\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.75, random_state=42)\n\nX_test = test_full2.drop([], axis=1,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CatBoost Regression\n\nparams = {\n    'iterations': 450,\n    'learning_rate': 0.1,\n    'l2_leaf_reg': 2,\n    'eval_metric': 'AUC',\n    'loss_function': 'RMSE',\n    'use_best_model': True,\n    'random_seed': 42,\n    'logging_level': 'Silent'\n}\n\n\nmodel = CatBoostRegressor(**params)\n\n# categorical_features_names = [0]\n\n# Fit model\nmodel.fit(\n    X_train, y_train,\n#     cat_features=categorical_features_names,\n    eval_set=(X_validation, y_validation),\n#     logging_level='Verbose',  # you can uncomment this for text output\n    plot=True\n);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import Pool, CatBoostRegressor, cv\n\nfeature_score = pd.DataFrame(list(zip(X.dtypes.index, model.get_feature_importance(Pool(X_train, label=y_train)))),\n                columns=['Feature','Score'])\n\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nfeature_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def del_bag_feat_2(data):\n\n    feature_score_upd = feature_score[(feature_score['Score']<2)]\n    feature_score_upd = feature_score_upd['Feature'].tolist()\n    \n    print(feature_score_upd)\n    \n    indexes_to_keep = set(data.columns) - set(feature_score_upd)    \n   \n    data = data[indexes_to_keep]\n    \n\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full2 = del_bag_feat_2(train_full2)\ntest_full2 = del_bag_feat_2(test_full2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_full2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predictions\npredictions_catboost = model.predict(X_test)\nprint(predictions_catboost[:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (12,7)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\n\nrects = ax.patches\n\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CatBoostRegressor(\n    l2_leaf_reg=int(best['l2_leaf_reg']),\n    learning_rate=best['learning_rate'],\n    iterations=750,\n    eval_metric='AUC',\n    random_seed=42,\n    verbose=True,\n    loss_function='RMSE',\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_data = cv(Pool(X_train, y_train), model.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cv_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predictions\npredictions_catboost = model.predict(X_test)\nprint(predictions_catboost[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь хочу просто перебрать много вариантов предсказаний катбуста с разными параметрами и просто усредить и посмотреть, что получится."},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_STATE = 900\n\nCATBOOST_PARAMS_GENERATOR_VER = 11\n\ncatbr_params = {\n    'random_seed': RANDOM_STATE, \n    'learning_rate': 0.1, #0.2-оптимальный параметр\n    'iterations': 750, #750-оптимальный параметр\n    'depth': 9, # 9-оптимальный параметр\n    'l2_leaf_reg': 2, # 2-оптимальный параметр\n    'eval_metric': 'AUC',\n    'subsample' : 0.75,\n    'random_strength': 0.06, \n    'od_type': \"Iter\", #детектор переобучения catboost\n    'od_wait': 1500,  # 1500-оптимальный параметр \n    'loss_function': 'RMSE',\n    'verbose': False, \n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ncol_mean = []\n\nresult_pred_df = pd.DataFrame()\n\nfor i in range(30):\n    # Params rendomizer\n    catbr_params['subsample'] = random.choice([0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n    catbr_params['learning_rate'] = random.choice([0.3, 0.1, 0.2, 0.4])\n    catbr_params['depth'] = random.choice([2, 4, 6, 7, 8, 9])\n    catbr_params['l2_leaf_reg'] = random.choice([0.85, 0.9, 1, 2, 3, 4, 2.5, 3.5, 4.5])\n    catbr_params['random_seed'] = random.randint(10,10000)\n    predict_model_ = 'result' + str(CATBOOST_PARAMS_GENERATOR_VER) + str(i) + \"_subs_\" + str(catbr_params['subsample'])+  \"_lr_\" + str(catbr_params['learning_rate'])+ \"_l2_\" + str(catbr_params['l2_leaf_reg'])+\\\n            \"_rs_\" + str(catbr_params['random_seed'])\n      \n    # Model\n    model = CatBoostRegressor(**catbr_params)\n    \n    # Model fit\n    model.fit(\n    X_train, y_train,\n    eval_set=(X_validation, y_validation),\n#     cat_features=categorical_features_names,\n    plot=False\n    );\n    \n    print(i)\n    \n    # Get predictions\n    predict_model_ = model.predict(X_test)\n    result_pred_df[f'result_{i}'] = predict_model_\n    print(predict_model_[:10])    \n\n    col_mean.extend([predict_model_])\n    \nprint(col_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_list = result_pred_df.columns.tolist()\nresult_pred_df['y_pred_v0'] = result_pred_df[col_list].mean(axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_pred_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_catboost = result_pred_df['y_pred_v0'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_catboost.min(), predictions_catboost.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_predictions_catboost = (predictions_catboost - predictions_catboost.min())/(predictions_catboost - predictions_catboost.min()).max()\nnormalized_predictions_catboost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds = (normalized_predictions_catboost + test_normalized_preds_lightfm) / 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['rating'] = normalized_preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VERSION = 45","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(f'submission_log_v{VERSION}.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN_collab_filter"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12,8)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, accuracy_score, f1_score, precision_score, recall_score\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential, Model\nfrom keras.layers import Embedding, Flatten, Dense, Dropout, concatenate, multiply, Input, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import metrics\nfrom keras.utils.vis_utils import plot_model\n\nimport scikitplot as skplt\n\nimport sys\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \nRANDOM_SEED = 13\n\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Pandas       :', pd.__version__)\nprint('Numpy        :', np.__version__)\nprint('Keras        :', keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def open_data():\n    \"\"\" open datasets\"\"\"\n    global train, test, sample_submission # объявляем переменные глобальными\n    train = pd.read_csv('/kaggle/input/recommendationsv4/train.csv', low_memory = False)\n    train = train.drop_duplicates().reset_index(drop = True) # удалим дубликаты, если есть\n    test = pd.read_csv('/kaggle/input/recommendationsv4/test.csv', low_memory = False)\n    sample_submission = pd.read_csv('/kaggle/input/recommendationsv4/sample_submission.csv')\n    \nopen_data() # открываем все и записываем датасет в переменные\n\ndef param_data(data): # посмотрим на данные\n    \"\"\"dataset required parameters \"\"\"\n    param = pd.DataFrame({\n              'dtypes': data.dtypes.values,\n              'nunique': data.nunique().values,\n              'isna': data.isna().sum().values,\n              'loc[0]': data.loc[0].values,\n              }, \n             index = data.loc[0].index)\n    return param\n\npd.concat([param_data(train), param_data(test)], \n          axis=1, \n          keys = [f'↓ ОБУЧАЮЩАЯ ВЫБОРКА ↓ {train.shape}', f'↓ ТЕСТОВАЯ ВЫБОРКА ↓ {test.shape}'],  \n          sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def viz_na(data):\n    \"\"\"NA visualisation\"\"\"\n    global cols\n    cols = data.columns # запишем названия строки сделаем переменную глобальной\n    # определяем цвета \n    # желтый - пропущенные данные, синий - не пропущенные\n    colours = ['#000099', '#ffff00'] \n    sns.heatmap(data[cols].isnull(), cmap=sns.color_palette(colours))\n    plt.show()\n\n\nviz_na(train)\nviz_na(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stat_na_per_percent(data):\n    print(f'{data.shape}')\n    for col in data.columns:\n        pct_missing = np.mean(data[col].isnull())\n        print('{} - {}%'.format(col, round(pct_missing*100)))\n    print(\"END\", end = '\\n\\n')\nstat_na_per_percent(train)\nstat_na_per_percent(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['rating']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_GB = pd.DataFrame({\n    'userid': train['userid'],\n    'itemid': train['itemid'],\n    'rating': train['rating']\n})\n\ntest_data_GB = pd.DataFrame({\n    'userid': test['userid'],\n    'itemid': test['itemid'],\n})\n\ntrain_data_GB = pd.get_dummies(train_data_GB, prefix='', prefix_sep='', columns=['rating'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data_GB['rating'] = train_data_GB['rating'].astype(int)\n## unisue users, books\nuserid, utemid = len(train_data_GB.userid.unique()), len(train_data_GB.itemid.unique())\n\n\nf'The dataset includes {len(train_data_GB)} ratings by {userid} unique users on {utemid} unique itemid.'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_, test_ = train_test_split(train_data_GB, test_size=0.01)\nf\"The training and testing data include {len(train), len(test)} records.\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tabular data method"},{"metadata":{"trusted":true},"cell_type":"code","source":"keras.backend.clear_session()\n## define the number of latent factors (can be different for the users and books)\ndim_embedding_user = 40\ndim_embedding_book = 40\n\n## book embedding\nutem_input= Input(shape=[1], name='Utem')\nutem_embedding = Embedding(utemid + 1, dim_embedding_book, name='Utem-Embedding')(utem_input)\nutem_vec = Flatten(name='Utem-Flatten')(utem_embedding)\nutem_vec = Dropout(0.5)(utem_vec)\n\n## user embedding\nuser_input = Input(shape=[1], name='User')\nuser_embedding = Embedding(userid + 1, dim_embedding_user, name ='User-Embedding')(user_input)\nuser_vec = Flatten(name ='User-Flatten')(user_embedding)\nuser_vec = Dropout(0.5)(user_vec)\n\n## concatenate flattened values \nconcat = concatenate([utem_vec, user_vec])\nconcat_dropout = Dropout(0.5)(concat)\n\n## add dense layer (can try more)\ndense_1 = Dense(30, name ='Fully-Connected1', activation='relu')(concat)\ndense_1 = Dense(30, name ='Fully-Connected2', activation='relu')(dense_1)\ndense_1 = Dense(30, name ='Fully-Connected3', activation='relu')(dense_1)\ndense_1 = Dense(30, name ='Fully-Connected4', activation='relu')(dense_1)\n## define output (can try sigmoid instead of relu)\nresult = Dense(2, activation ='softmax',name ='Activation')(dense_1)\n\n## define model with 2 inputs and 1 output\nmodel_tabular = Model([user_input, utem_input], result)\nmodel_tabular.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model_tabular, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## specify learning rate (or use the default by specifying optimizer = 'adam')\nopt_adam = Adam(lr = 0.0003)\n\n## compile model\nmodel_tabular.compile(optimizer= opt_adam, loss= ['binary_crossentropy'], metrics=[keras.metrics.AUC()])\n\n## fit model\nhistory_tabular = model_tabular.fit([train_['userid'], train_['itemid']],\n                                    train_.iloc[:,[-2,-1]],\n                                    batch_size = 1000,\n                                    validation_split = 0.1,\n                                    epochs = 4,\n                                    verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_GB = model_tabular.predict([test_data_GB['userid'], test_data_GB['itemid']])[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = model_tabular.predict([test_['userid'], test_['itemid']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skplt.metrics.plot_roc(test_.iloc[:, -1], test_pred)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, _ = roc_curve(test_.iloc[:, -1], test_pred[:,1])\nauc = roc_auc_score(test_.iloc[:,  -1], test_pred[:, 1])\naccuracy = accuracy_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\nf1 = f1_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\nprecision =  precision_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\nrecall = recall_score(test_.iloc[:, -1], np.argmax(test_pred, axis = -1))\n\nprint('FP, TP              :', sum(fpr), sum(tpr))\nprint('ROC_AUC_SCORE       :', auc)\nprint('accuracy            :', accuracy)\nprint('precision           :', precision)\nprint('recall              :', recall)\nprint('f1                  :', f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['rating'] = test_pred_GB\nsample_submission.to_csv('submission_XXXXXX.csv', index=False)\nsample_submission.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Эмбединги\nutem_embedding_weights = model_tabular.layers[2].get_weights()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"utem_embedding_weights","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}